# -*- coding: utf-8 -*-
"""Copy of transformer ar2en (LDC).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1du52gJa1S_tffOKkITdMoMKED_w3xwTI

# Transformer model for language understanding
"""
import os 

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional
import tkseem as tk
import time
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import json
import tnkeeh as tn
from bpe import *
import argparse
import logging 
import sys


parser = argparse.ArgumentParser(description='Args')
parser.add_argument('--tok', type=int)
parser.add_argument('--run', type=int)
parser.add_argument('--vocab_size', type=int)
parser.add_argument('--dir', type=str)
parser.add_argument('--max_word_tokens', type=int)

args = parser.parse_args()

"""## Setup input pipeline"""

MAX_TOKENS = args.max_word_tokens

train_text = open('/content/train_data.txt', 'r').read().splitlines()
train_lbls = [int(lbl) for lbl in open('/content/train_labels.txt', 'r').read().splitlines()]
valid_text = open('/content/valid_data.txt', 'r').read().splitlines()
valid_lbls = [int(lbl) for lbl in open('/content/valid_labels.txt', 'r').read().splitlines()]
test_text = open('/content/test_data.txt', 'r').read().splitlines()
test_lbls = [int(lbl) for lbl in open('/content/test_labels.txt', 'r').read().splitlines()]

assert len(train_text) == len(train_lbls)
assert len(test_text) == len(test_text)

def tokenize_data(tokenizer, vocab_size = 10000):
  train_data = tokenizer.encode(train_text, out_len=MAX_TOKENS)
  valid_data = tokenizer.encode(valid_text, out_len=MAX_TOKENS)
  test_data = tokenizer.encode(test_text, out_len=MAX_TOKENS)
  return tokenizer, (train_data, train_lbls), (valid_data, valid_lbls), (test_data, test_lbls)

def create_dataset(train_data, valid_data, test_data, batch_size = 256, buffer_size = 50000):
  train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(buffer_size)
  train_dataset = train_dataset.batch(batch_size, drop_remainder=True)

  valid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)
  valid_dataset = valid_dataset.batch(batch_size)

  test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
  test_dataset = test_dataset.batch(batch_size)
  return train_dataset, valid_dataset, test_dataset


def create_model(vocab_size):
    model = Sequential()
    model.add(Embedding(vocab_size, 128))
    model.add(Bidirectional(GRU(units = 256, return_sequences = True)))
    model.add(Bidirectional(GRU(units = 256)))
    model.add(Dense(1, activation = 'sigmoid'))
    return model

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False,
              reduction = 'none')

def loss_function(real, pred):
  loss_ = loss_object(real, pred)
  return tf.reduce_mean(loss_)

def accuracy_function(real, pred):
  result = tf.equal(tf.squeeze(real),tf.squeeze(tf.cast(tf.round(pred), tf.int32))) 
  return tf.reduce_mean( tf.cast(result, tf.float32))

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')

valid_loss = tf.keras.metrics.Mean(name='valid_loss')
valid_accuracy = tf.keras.metrics.Mean(name='valid_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.Mean(name='test_accuracy')

# @tf.function(input_signature=train_step_signature)
def test_step(inp, tar):
  predictions = model(inp)
  loss = loss_function(tar, predictions)
  
  test_loss(loss)
  test_accuracy(accuracy_function(tar, predictions))


def evaluate_test(test_dataset):
  test_loss.reset_states()
  test_accuracy.reset_states()
  for (batch, (inp, tar)) in enumerate(test_dataset):
      test_step(inp, tar)

  return test_loss.result().numpy(), test_accuracy.result().numpy()

results = {}

BATCH_SIZE = 256
vocab_size = args.vocab_size
j = args.run 
i = args.tok

checkpoint_dir = f'{args.dir}/ckpts/'
tokenizers = [bpe(vocab_size, lang = 'ar'), bpe(vocab_size, lang = 'ar', morph = True)]
tokenizer = tokenizers[i]
name = tokenizer.name
tok_dir = f"{checkpoint_dir}/vocab_size_{vocab_size}/{name}"
print('loading pretrained tokenizer')
tokenizer.load(f"{tok_dir}")
tokenizer, train_data, valid_data, test_data = tokenize_data(tokenizer, vocab_size = vocab_size)
train_dataset, valid_dataset, test_dataset = create_dataset(train_data, valid_data, test_data, batch_size = BATCH_SIZE)
optimizer = tf.keras.optimizers.Adam()
model = create_model(vocab_size)
checkpoint = tf.train.Checkpoint(
        optimizer=optimizer,
        model=model
)
ckpt_manager = tf.train.CheckpointManager(
    checkpoint,
    f'{checkpoint_dir}/vocab_size_{vocab_size}/{name}/run_{j}',
    max_to_keep=1,
    checkpoint_name='ckpt',
)

# restore best model
checkpoint.restore(ckpt_manager.latest_checkpoint)
_, test_score = evaluate_test(test_dataset)
print('results on test score is:',test_score)
